{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection Legislation Data\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Maryland's General Assembly provides a lot of different data concerning the legislative process. There's an [excel file of all proposed bills](), [information on each legislator](), and [even a fiscal analysis of each bill proposed](). In this scraper, I'll be gathering information from each bill proposed in the 2018 sessions.\n",
    "\n",
    "There's a good deal of data available on the bill's \"homepage\".\n",
    "\n",
    "![web page of bill](documents\\bill_homepage.PNG)\n",
    "\n",
    "In addition to information on the website, each bill is published as a PDF. That PDF contains the text of the bill, and the new legislation or the legislation that will be repealed.\n",
    "\n",
    "![PDF of bill](documents\\bill_pdf.PNG)\n",
    "\n",
    "For this topic modeling project, I'm more interested in the actual text of the bill. The additional information [could help improve the topic modeling process](https://link.springer.com/chapter/10.1007/978-3-642-40501-3_39), but I'm going to start simple and just use the text of the bill. Luckily, each bill begins with a sort of \"executive summary\", which is what I'll use for my topic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "You can find the requirements for the entire project in the home directory of this project, however I'd also like to include a list of the tools I used to collect the data\n",
    "\n",
    "* **Scrapy**==1.4.0\n",
    "* **pdfminer.six**==20170720\n",
    "* **requests**==2.18.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Web Scraping Script\n",
    "\n",
    "\n",
    "### Defining Items\n",
    "\n",
    "There are a few different web scrapping tools out there, but I prefer `Scrapy` and that's what I'll be doing here. After using `scrapy startproject` to set up my work environment, I'm first going to define the items- what data I'll be collecting from the web scrape.\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class LegislationScraperItem(scrapy.Item):\n",
    "    bill_name = scrapy.Field()\n",
    "    bill_number = scrapy.Field()\n",
    "    url = scrapy.Field()\n",
    "    sponsor = scrapy.Field()\n",
    "    status = scrapy.Field()\n",
    "    committee = scrapy.Field()\n",
    "    broad_subjects = scrapy.Field()\n",
    "    narrow_subjects = scrapy.Field()\n",
    "    purpose = scrapy.Field()\n",
    "```\n",
    "\n",
    "I want to collect all the data from each bill web page, as well as the \"executive summary\" of the bill that I mentioned in the introduction. I'll call this the \"purpose\" of the bill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Spider\n",
    "\n",
    "Now I'm going to build out the main part of the spider. This is the script that will point the web crawler towards the data we want to collect, and define how the spider should move through the website.\n",
    "\n",
    "First, I'll import some requires packages and create the spider object. When creating the spider object, I need to give it a name, define the allowed domains, and provide some start URLs. Since I want to get information on every bill that was proposed last sessions, I'm going to a web page that that show each bill proposed in a given committee, and include every committee's web page in that list.\n",
    "\n",
    "![web page highlighting bills proposed in a committee](documents\\committee_bills.PNG)\n",
    "```python\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from legislation_scraper.items import LegislationScraperItem\n",
    "\n",
    "\n",
    "class LegislationSpider(CrawlSpider):\n",
    "    name = 'legislation_spider'\n",
    "    allowed_domains = ['mgaleg.maryland.gov']\n",
    "    start_urls = [\n",
    "        'http://mgaleg.maryland.gov/webmga/frmMain.aspx?id=app&stab=02&pid=cmtepage&tab=subject3&ys=2018RS',\n",
    "        'http://mgaleg.maryland.gov/webmga/frmMain.aspx?id=b%26t&stab=02&pid=cmtepage&tab=subject3&ys=2018RS',\n",
    "        'http://mgaleg.maryland.gov/webmga/frmMain.aspx?id=ecm&stab=02&pid=cmtepage&tab=subject3&ys=2018RS',\n",
    "        'http://mgaleg.maryland.gov/webmga/frmMain.aspx?id=ehe&stab=02&pid=cmtepage&tab=subject3&ys=2018RS',\n",
    "        'http://mgaleg.maryland.gov/webmga/frmMain.aspx?id=env&stab=02&pid=cmtepage&tab=subject3&ys=2018RS',\n",
    "        'http://mgaleg.maryland.gov/webmga/frmMain.aspx?id=exn&stab=02&pid=cmtepage&tab=subject3&ys=2018RS',\n",
    "        'http://mgaleg.maryland.gov/webmga/frmMain.aspx?id=fin&stab=02&pid=cmtepage&tab=subject3&ys=2018RS',\n",
    "        'http://mgaleg.maryland.gov/webmga/frmMain.aspx?id=hgo&stab=02&pid=cmtepage&tab=subject3&ys=2018RS',\n",
    "        'http://mgaleg.maryland.gov/webmga/frmMain.aspx?id=jpr&stab=02&pid=cmtepage&tab=subject3&ys=2018RS',\n",
    "        'http://mgaleg.maryland.gov/webmga/frmMain.aspx?id=jud&stab=02&pid=cmtepage&tab=subject3&ys=2018RS',\n",
    "        'http://mgaleg.maryland.gov/webmga/frmMain.aspx?id=sru&stab=02&pid=cmtepage&tab=subject3&ys=2018RS',\n",
    "        'http://mgaleg.maryland.gov/webmga/frmMain.aspx?id=hru&stab=02&pid=cmtepage&tab=subject3&ys=2018RS',\n",
    "        'http://mgaleg.maryland.gov/webmga/frmMain.aspx?id=w%26m&stab=02&pid=cmtepage&tab=subject3&ys=2018RS'\n",
    "    ]\n",
    "```\n",
    "\n",
    "With these start URLs, I have access to each bill proposed in 2018, but I'll need to define a path for the spider to follow in order to access the bill's web page and PDF.\n",
    "\n",
    "The bill numbers ion this page are actually hyperlinks, and the spider could follow those links in order to access the relevant information. Using Xpaths, the Google Chrome developer tools, and a helpful addon called [Xpath Helper](https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl?hl=en), I was able to find and define an xpath that directs the spider to all the hyperlinks that navigate to a bill's web page.\n",
    "\n",
    "By including it below in the rules, and restricting the spider to only those xpaths, we can crawl the webpage and use the `parse_item` function to collect all the items I defined above.\n",
    "```python\n",
    "    rules = [\n",
    "        Rule(LinkExtractor(\n",
    "            restrict_xpaths='//table[@class = \"grid\"]//tr/td[1]/a[1]'), callback='parse_item', follow=True)\n",
    "    ]\n",
    "```\n",
    "\n",
    "The `parse_item` function uses xpaths to extract text information from the web page. It was created using the same tools I used to find the xpath above, along with some trial and error.\n",
    "\n",
    "```python\n",
    "    def parse_item(self, response):\n",
    "        item = LegislationScraperItem()\n",
    "        item['bill_name'] = response.xpath(\n",
    "            '//table[@class = \"billheader\"]//tr[1]/td[3]/h3/text()').extract()\n",
    "        item['bill_number'] = response.xpath(\n",
    "            '//table[@class = \"billheader\"]//tr[2]/td[1]/h2/a/text()').extract()\n",
    "        item['url'] = ''.join([\"http://mgaleg.maryland.gov\", response.xpath(\n",
    "            '//table[@class = \"billheader\"]//tr[2]/td[1]/h2/a/@href').extract()[0]])\n",
    "        item['sponsor'] = response.xpath(\n",
    "            '//table[@class = \"billheader\"]//tr[2]/td[3]/h3/a/text()').extract()\n",
    "        item['status'] = response.xpath(\n",
    "            '//table[@class = \"billheader\"]//tr[3]/td[3]/h3/text()').extract()\n",
    "        item['committee'] = response.xpath(\n",
    "            '//table[@class = \"subcomm\"]//td/a/text()').extract()\n",
    "        item['broad_subjects'] = response.xpath(\n",
    "            '//table[@class = \"billsum\"]//tr[descendant::text()[contains(.,\"Broad Subject\")]]/td/a/text()').extract()\n",
    "        item['narrow_subjects'] = response.xpath(\n",
    "            '//table[@class = \"billsum\"]//tr[descendant::text()[contains(.,\"Narrow Subject\")]]/td/a/text()').extract()\n",
    "        yield item\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Item Pipeline\n",
    "\n",
    "The only item I haven't collected is `purpose`. Remember, this data is actually text that is only found in the bills text which is save as a PDF; it's not available in full on the website. Luckily, we can use `pdfminer` to extract this information (you'll need to use `pdfminer.six` if you're on Python 3.x like I am). \n",
    "\n",
    "I'm going to use the `pipeline` module from `Scrapy` and `pdf2txt` to extract the `purpose` from each document.\n",
    "\n",
    "In building to the `process_item` function , the first step is to call the URL that I collected from the bill homepage.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "\n",
    "\n",
    "class LegislationScraperPipeline(object):\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        url = item['url']\n",
    "        bill_number = item['bill_number'][0]\n",
    "        r = requests.get(url, stream=True)\n",
    "```\n",
    "\n",
    "`r` contains the PDF for a specific bill. I'll need to save the content to a PDF file and then convert it to a `txt` file using `pdf2txt`. I'm going to save the PDF file in the `data` folder and name it according to the bill name.\n",
    "\n",
    "```python\n",
    "        pdf_filename = 'data\\\\' + bill_number + '.pdf'\n",
    "        txt_filename = 'data\\\\' + bill_number + '.txt'\n",
    "        with open(pdf_filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "```\n",
    "\n",
    "Now, there isn't a module I can use in a python script to call `pdf2txt`. Instead, I'm going to use `os.system` to call `pdf2txt` as if I were running it in the command shell.\n",
    "\n",
    "```python\n",
    "        pdf2txt_path = 'legislation_scraper/pdf2txt.py'\n",
    "        print(\"python \" + pdf2txt_path + \" -o \" +\n",
    "              txt_filename + \" \" + pdf_filename)\n",
    "        os.system(\"python \" + pdf2txt_path + \" -o \" +\n",
    "                  txt_filename + \" \" + pdf_filename)\n",
    "```\n",
    "\n",
    "Now I'm going to use a regular expression to extract the purpose from the bill text and save it as a text file.\n",
    "\n",
    "I'm adding in a `while` statement to wait until the text file is written.\n",
    "\n",
    "Finally, I'm closing all the files just to wrap things up.\n",
    "\n",
    "```python\n",
    "        while not os.path.exists(txt_filename):\n",
    "            time.sleep(0.001)\n",
    "        file = open(txt_filename, 'r', encoding='utf8')\n",
    "        filetext = file.read()\n",
    "        filetext = filetext.replace('\\n', '')\n",
    "        filetext = filetext.replace('\\r', '')\n",
    "        filetext = ''.join(filetext)\n",
    "        mat = re.search(\n",
    "            r'(?=(FOR|AN ACT)).+?(?=( BY|(|\\(1\\) )SECTION 1.| WHEREAS| EXPLANATION))', filetext)\n",
    "        if mat:\n",
    "            item['purpose'] = ''.join(mat.group(0))\n",
    "        else:\n",
    "            item['purpose'] = \"\"\n",
    "        file.close()\n",
    "        os.remove(pdf_filename)\n",
    "        os.remove(txt_filename)\n",
    "        return item\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Scraper Settings\n",
    "\n",
    "Finally, I need to change some of the setting to make sure the spider and pipeline are enabled, and ensure that I don't flood the website with requests\n",
    "\n",
    "```python\n",
    "BOT_NAME = 'legislation_scraper'\n",
    "\n",
    "SPIDER_MODULES = ['legislation_scraper.spiders']\n",
    "NEWSPIDER_MODULE = 'legislation_scraper.spiders'\n",
    "\n",
    "ROBOTSTXT_OBEY = True\n",
    "\n",
    "ITEM_PIPELINES = {\n",
    "    'legislation_scraper.pipelines.LegislationScraperPipeline': 300,\n",
    "}\n",
    "\n",
    "AUTOTHROTTLE_ENABLED = True\n",
    "\n",
    "AUTOTHROTTLE_START_DELAY = 5\n",
    "\n",
    "AUTOTHROTTLE_MAX_DELAY = 60\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the Spider\n",
    "\n",
    "Finally, we'll open up a command prompt and run the following command to save the data as a csv file.\n",
    "\n",
    "scrapy crawl legislation_scraper -o data.csv -t csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
